{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.wrn import WideResNet\n",
    "import utils.attacks as attacks\n",
    "from utils.detector import Detector, style_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Trains a CIFAR Classifier',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--dataset', '-d', type=str, default='cifar10', choices=['cifar10', 'cifar100'],\n",
    "                    help='Choose between CIFAR-10, CIFAR-100.')\n",
    "parser.add_argument('--model', '-m', type=str, default='wrn',\n",
    "                    choices=['allconv', 'wrn'], help='Choose architecture.')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', '-e', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The initial learning rate.')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--test_bs', type=int, default=256)\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "# WRN Architecture\n",
    "parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n",
    "parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n",
    "parser.add_argument('--droprate', default=0.0, type=float, help='dropout probability')\n",
    "# Checkpoints\n",
    "parser.add_argument('--save', '-s', type=str, default='./snapshots/baseline', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--load', '-l', type=str, default='', help='Checkpoint path to resume / test.')\n",
    "parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--gpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "args = parser.parse_args([\"--save\", \"checkpoints/\", \"--gpu\", \"3\", \"--test_bs\", \"128\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "print(state)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# # mean and standard deviation of channels of CIFAR-10 images\n",
    "# mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "# std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n",
    "                               trn.ToTensor()])\n",
    "test_transform = trn.Compose([trn.ToTensor()])\n",
    "\n",
    "train_data = dset.CIFAR10('~/datasets/cifarpy', train=True, transform=train_transform, download=True)\n",
    "test_data = dset.CIFAR10('~/datasets/cifarpy', train=False, transform=test_transform, download=True)\n",
    "num_classes = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=args.test_bs, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "normalize = trn.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "detector_data_transform = trn.Compose([trn.ToTensor(), normalize])\n",
    "\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if args.model == 'allconv':\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n",
    "    net.load_state_dict(torch.load(\"benchmark_ckpts/cifar10_reg_training_99.pt\"))\n",
    "    start_epoch = 80\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Restore model if desired\n",
    "if args.load != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(args.load, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "if args.ngpu > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n",
    "\n",
    "if args.ngpu > 0:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    net.cuda()\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args.epochs * len(train_loader),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args.learning_rate))\n",
    "\n",
    "\n",
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg, loss_gram_avg = 0.0, 0.0\n",
    "    \n",
    "    for bx, by in tqdm(train_loader):\n",
    "        bx, by = bx.cuda(), by.cuda()\n",
    "\n",
    "        adv_bx = adversary(net, bx, by)\n",
    "                        \n",
    "        # forward\n",
    "        logits_reg, feats_reg = net.gram_forward(bx * 2 - 1)\n",
    "        logits_adv, feats_adv = net.gram_forward(adv_bx * 2 - 1)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_reg = F.cross_entropy(logits_reg, by)\n",
    "#         loss_adv = F.cross_entropy(logits_adv, by)\n",
    "        \n",
    "        loss_gram = style_loss(feats_reg, feats_adv).cuda()\n",
    "        loss = loss_reg + F.relu(10 - loss_gram)\n",
    "        \n",
    "        print(loss)\n",
    "        print(loss_reg, loss_gram)\n",
    "                        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "        loss_gram_avg = loss_gram_avg * 0.8 + float(loss_gram) * 0.2\n",
    "    \n",
    "    state['train_loss'] = loss_avg\n",
    "    state[\"gram_train_loss\"] = loss_gram_avg\n",
    "    \n",
    "    print(\"Train Loss:\", state[\"train_loss\"])\n",
    "    print(\"Train Gram: \", state[\"gram_train_loss\"])\n",
    "\n",
    "# test function\n",
    "def test(detector = None):\n",
    "    net.eval()\n",
    "    loss_avg, loss_reg_avg, loss_adv_avg, loss_gram_avg, auroc_avg = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss, loss_reg, loss_adv, loss_gram = 0.0, 0.0, 0.0, 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            adv_data = adversary(net, data, target)\n",
    "\n",
    "            # forward\n",
    "            output = net(data * 2 - 1)\n",
    "#             adv_output = net(adv_data * 2 - 1)\n",
    "            \n",
    "#             loss_reg = F.cross_entropy(output, target)\n",
    "#             loss_adv = F.cross_entropy(adv_output, target)\n",
    "            \n",
    "#             loss = loss_reg\n",
    "            \n",
    "            auroc = detector.compute_ood_deviations_batch(adv_data * 2 -1)\n",
    "            auroc = auroc[\"AUROC\"]\n",
    "            \n",
    "#             loss_gram = gram_margin_loss(feats_reg, feats_adv, margin=10e4)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).sum().item()\n",
    "\n",
    "            # test loss average\n",
    "#             loss_reg_avg += float(loss_reg)\n",
    "#             loss_adv_avg += float(loss_adv)\n",
    "#             loss_avg += float(loss)\n",
    "#             loss_gram_avg += float(loss_gram)\n",
    "            auroc_avg += auroc\n",
    "            \n",
    "            \n",
    "    state['test_loss_reg'] = loss_reg_avg / len(test_loader)\n",
    "    state['test_loss_adv'] = loss_adv_avg / len(test_loader)\n",
    "    state['test_loss'] = loss_avg / len(test_loader)\n",
    "    state['test_accuracy'] = correct / len(test_loader.dataset)\n",
    "    state['gram_loss'] = loss_gram_avg / len(test_loader)\n",
    "    state[\"gram_auroc\"] = auroc_avg / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414c1bd640cb42f9af55f64e595e7bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1040, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.1981, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.0940, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(11.1059, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2121, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.1061, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(11.1292, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2555, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.1263, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(11.1846, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3424, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.1578, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(11.0858, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2891, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.2033, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(11.1051, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3775, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.2724, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(10.7378, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.1373, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.3995, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(10.4003, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2855, device='cuda:3', grad_fn=<NllLossBackward>) tensor(0.8852, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.8700, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.8700, device='cuda:3', grad_fn=<NllLossBackward>) tensor(782.6428, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.7873, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.7873, device='cuda:3', grad_fn=<NllLossBackward>) tensor(1190.9128, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4929, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.4929, device='cuda:3', grad_fn=<NllLossBackward>) tensor(542.4374, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4764, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.4764, device='cuda:3', grad_fn=<NllLossBackward>) tensor(813.1430, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3813, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3813, device='cuda:3', grad_fn=<NllLossBackward>) tensor(907.3920, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3163, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3163, device='cuda:3', grad_fn=<NllLossBackward>) tensor(1883.8151, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5473, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.5473, device='cuda:3', grad_fn=<NllLossBackward>) tensor(792.6848, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4224, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.4224, device='cuda:3', grad_fn=<NllLossBackward>) tensor(1405.0215, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3647, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3647, device='cuda:3', grad_fn=<NllLossBackward>) tensor(706.4724, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2670, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2670, device='cuda:3', grad_fn=<NllLossBackward>) tensor(889.0773, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2242, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2242, device='cuda:3', grad_fn=<NllLossBackward>) tensor(452.7786, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3682, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.3682, device='cuda:3', grad_fn=<NllLossBackward>) tensor(601.3641, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2245, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2245, device='cuda:3', grad_fn=<NllLossBackward>) tensor(364.2177, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1915, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.1915, device='cuda:3', grad_fn=<NllLossBackward>) tensor(449.5688, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2409, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.2409, device='cuda:3', grad_fn=<NllLossBackward>) tensor(460.8798, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1675, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.1675, device='cuda:3', grad_fn=<NllLossBackward>) tensor(241.3837, device='cuda:3', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1435, device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor(1.1435, device='cuda:3', grad_fn=<NllLossBackward>) tensor(265.8396, device='cuda:3', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c95d7c1ce33f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1. Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Initializing Detector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6e64583ea159>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/home/joelsimonoff/remote/main/lib/python3.5/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/home/joelsimonoff/remote/main/lib/python3.5/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make save directory\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.isdir(args.save):\n",
    "    raise Exception('%s is not a dir' % args.save)\n",
    "\n",
    "with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%),gram_auroc\\n')\n",
    "\n",
    "print('Beginning Training!\\n')\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "    \n",
    "    print(\"1. Training\")\n",
    "    train()\n",
    "    print(\"2. Initializing Detector\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        net.eval()\n",
    "        detector = Detector(net, data_train, data_test, args.test_bs, pbar=None)\n",
    "        print(\"3. Testing\")\n",
    "        try:\n",
    "            test(detector)\n",
    "        except Exception as e:\n",
    "            print(\"Failed test\")\n",
    "            print(e)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(net.state_dict(),\n",
    "                   os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                '_baseline_epoch_' + str(epoch) + '.pt'))\n",
    "        # Let us not waste space and delete the previous model\n",
    "        prev_path = os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                 '_baseline_epoch_' + str(epoch - 1) + '.pt')\n",
    "        if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "        # Show results\n",
    "\n",
    "        with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                          '_baseline_training_results.csv'), 'a') as f:\n",
    "            f.write('%03d,%05d,%0.6f,%0.5f,%0.2f,%0.2f\\n' % (\n",
    "                (epoch + 1),\n",
    "                time.time() - begin_epoch,\n",
    "                state['train_loss'],\n",
    "                state['test_loss'],\n",
    "                100 - 100. * state['test_accuracy'],\n",
    "                state[\"gram_auroc\"],\n",
    "            ))\n",
    "\n",
    "        # # print state with rounded decimals\n",
    "#         print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n",
    "\n",
    "        print('Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f} | Gram Auroc {5:.2f}'.format(\n",
    "            (epoch + 1),\n",
    "            int(time.time() - begin_epoch),\n",
    "            state['train_loss'],\n",
    "            state['test_loss'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "            state[\"gram_auroc\"])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
