{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.wrn import WideResNet\n",
    "import utils.attacks as attacks\n",
    "from utils.detector import Detector, gram_margin_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Trains a CIFAR Classifier',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--dataset', '-d', type=str, default='cifar10', choices=['cifar10', 'cifar100'],\n",
    "                    help='Choose between CIFAR-10, CIFAR-100.')\n",
    "parser.add_argument('--model', '-m', type=str, default='wrn',\n",
    "                    choices=['allconv', 'wrn'], help='Choose architecture.')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', '-e', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The initial learning rate.')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--test_bs', type=int, default=256)\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "# WRN Architecture\n",
    "parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n",
    "parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n",
    "parser.add_argument('--droprate', default=0.0, type=float, help='dropout probability')\n",
    "# Checkpoints\n",
    "parser.add_argument('--save', '-s', type=str, default='./snapshots/baseline', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--load', '-l', type=str, default='', help='Checkpoint path to resume / test.')\n",
    "parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--gpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "args = parser.parse_args([\"--save\", \"checkpoints_margin_0001/\", \"--gpu\", \"2\", \"-b\", \"256\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ngpu': 1, 'load': '', 'droprate': 0.0, 'decay': 0.0005, 'batch_size': 256, 'layers': 40, 'dataset': 'cifar10', 'save': 'checkpoints_margin_0001/', 'gpu': 2, 'learning_rate': 0.1, 'test_bs': 256, 'model': 'wrn', 'test': False, 'epochs': 100, 'momentum': 0.9, 'prefetch': 2, 'widen_factor': 2}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "print(state)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# # mean and standard deviation of channels of CIFAR-10 images\n",
    "# mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "# std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n",
    "                               trn.ToTensor()])\n",
    "test_transform = trn.Compose([trn.ToTensor()])\n",
    "\n",
    "train_data = dset.CIFAR10('~/datasets/cifarpy', train=True, transform=train_transform, download=True)\n",
    "test_data = dset.CIFAR10('~/datasets/cifarpy', train=False, transform=test_transform, download=True)\n",
    "num_classes = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=args.test_bs, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "normalize = trn.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "detector_data_transform = trn.Compose([trn.ToTensor(), normalize])\n",
    "\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if args.model == 'allconv':\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n",
    "    net.load_state_dict(torch.load(\"benchmark_ckpts/cifar10_reg_training_99.pt\"))\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Restore model if desired\n",
    "if args.load != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(args.load, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "if args.ngpu > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n",
    "\n",
    "if args.ngpu > 0:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    net.cuda()\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args.epochs * len(train_loader),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 10\n",
    "margin_scale = 0.0001\n",
    "adversary = attacks.PGD_margin(epsilon=8./255, \n",
    "                               num_steps=10, \n",
    "                               step_size=2./255, \n",
    "                               margin=margin,\n",
    "                               margin_scale=margin_scale).cuda()\n",
    "\n",
    "def train():\n",
    "    net.train()\n",
    "    loss_avg, loss_gram_avg = 0.0, 0.0\n",
    "    i = 0\n",
    "    \n",
    "    for bx, by in tqdm(train_loader):\n",
    "        bx, by = bx.cuda(), by.cuda()\n",
    "                        \n",
    "        # forward\n",
    "        logits_reg, feats_reg = net.gram_forward(bx * 2 - 1)\n",
    "        \n",
    "        adv_bx = adversary(net, bx, by, feats_reg)\n",
    "        logits_adv, feats_adv = net.gram_forward(adv_bx * 2 - 1)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_reg = F.cross_entropy(logits_reg, by)\n",
    "#         loss_adv = F.cross_entropy(logits_adv, by)\n",
    "        \n",
    "        loss_gram = margin_scale * gram_margin_loss(feats_reg, feats_adv, margin=margin).cuda()\n",
    "        loss = loss_reg + loss_gram\n",
    "                                                \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "        loss_gram_avg = loss_gram_avg * 0.8 + float(loss_gram) * 0.2\n",
    "    \n",
    "    state['train_loss'] = loss_avg\n",
    "    state[\"gram_train_loss\"] = loss_gram_avg\n",
    "    \n",
    "    print(\"Train Loss:\", state[\"train_loss\"])\n",
    "    print(\"Train Gram: \", state[\"gram_train_loss\"])\n",
    "\n",
    "# test function\n",
    "def test(detector = None):\n",
    "    net.eval()\n",
    "        \n",
    "    acc_reg, acc_adv, auroc, auroc_failed = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for bx, by in tqdm(test_loader):\n",
    "            bx, by = bx.cuda(), by.cuda()\n",
    "\n",
    "            # forward\n",
    "            logits_reg, feats_reg = net.gram_forward(bx * 2 - 1)\n",
    "            \n",
    "            adv_bx = adversary(net, bx, by, feats_reg)\n",
    "            logits_adv, feats_adv = net.gram_forward(adv_bx * 2 - 1)\n",
    "            \n",
    "            logits_reg, logits_adv, by = logits_reg.cpu(), logits_adv.cpu(), by.cpu()\n",
    "            \n",
    "#             loss_reg = F.cross_entropy(logits_reg, by)\n",
    "#             print(loss_reg)\n",
    "            \n",
    "            a, a_f = detector.compute_auroc_advs(logits_adv, feats_adv, by)\n",
    "            auroc.append(a)\n",
    "            auroc_failed.append(a_f)\n",
    "            \n",
    "            acc_reg.append((by==torch.max(logits_reg,dim=1)[1]).cpu().numpy().mean())\n",
    "            acc_adv.append((by==torch.max(logits_adv,dim=1)[1]).cpu().numpy().mean())\n",
    "            \n",
    "    state['test_accuracy'] = np.mean(acc_reg)\n",
    "    state[\"adversarial_accuracy\"] = np.mean(acc_adv)\n",
    "    state['auroc'] = np.mean(auroc)\n",
    "    state[\"auroc_failed\"] = np.mean(auroc_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make save directory\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.isdir(args.save):\n",
    "    raise Exception('%s is not a dir' % args.save)\n",
    "\n",
    "with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%),gram_auroc\\n')\n",
    "\n",
    "print('Beginning Training!\\n')\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "    \n",
    "    print(\"1. Training\")\n",
    "    if epoch != 0:\n",
    "        train()\n",
    "    print(\"2. Initializing Detector\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        net.eval()\n",
    "        detector = Detector(net, data_train, data_test, args.test_bs, pbar=None)\n",
    "        print(\"3. Testing\")\n",
    "        try:\n",
    "            test(detector)\n",
    "        except Exception as e:\n",
    "            print(\"Failed test\")\n",
    "            print(e)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(net.state_dict(),\n",
    "                   os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                '_baseline_epoch_' + str(epoch) + '.pt'))\n",
    "        # Let us not waste space and delete the previous model\n",
    "        prev_path = os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                 '_baseline_epoch_' + str(epoch - 3) + '.pt')\n",
    "        if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "        # Show results\n",
    "\n",
    "#         with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "#                                           '_baseline_training_results.csv'), 'a') as f:\n",
    "#             f.write('%03d,%05d,%0.6f,%0.5f,%0.2f,%0.2f\\n' % (\n",
    "#                 (epoch + 1),\n",
    "#                 time.time() - begin_epoch,\n",
    "#                 state['train_loss'],\n",
    "#                 state['test_loss'],\n",
    "#                 100 - 100. * state['test_accuracy'],\n",
    "#                 state[\"gram_auroc\"],\n",
    "#             ))\n",
    "\n",
    "        # # print state with rounded decimals\n",
    "#         print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n",
    "\n",
    "        print('Epoch {0:3d} | Time {1:5d} | Adversarial Acc {2:.3f} | Test Error {3:.2f} | Auroc {4:.2f} | Auroc Failed {5:.2f}'.format(\n",
    "            (epoch + 1),\n",
    "            int(time.time() - begin_epoch),\n",
    "            100. * state['adversarial_accuracy'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "            state[\"auroc\"],\n",
    "            state[\"auroc_failed\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
