{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>WRN: Cifar10</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable, grad\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "import utils.calculate_log as callog\n",
    "from utils.wrn import WideResNet\n",
    "\n",
    "from utils.detector import Detector, gram_margin_loss\n",
    "import utils.attacks as attacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "torch_model = WideResNet(depth=40, widen_factor=2, num_classes=10)\n",
    "\n",
    "torch_model.load(path=\"benchmark_ckpts/cifar10_reg_training_99.pt\")\n",
    "# torch_model.load(path=\"benchmark_ckpts/cifar10_style_epoch_99.pt\")\n",
    "# torch_model.load(path=\"checkpoints_style_fine_tuning/cifar10_wrn_baseline_epoch_2.pt\")\n",
    "torch_model.cuda()\n",
    "torch_model.params = list(torch_model.parameters())\n",
    "torch_model.eval()\n",
    "print(\"Done\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In-distribution Datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# mean = np.array([[125.3/255, 123.0/255, 113.9/255]]).T\n",
    "\n",
    "# std = np.array([[63.0/255, 62.1/255.0, 66.7/255.0]]).T\n",
    "# normalize = transforms.Normalize((125.3/255, 123.0/255, 113.9/255), (63.0/255, 62.1/255.0, 66.7/255.0))\n",
    "\n",
    "normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        \n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.CenterCrop(size=(32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=True, download=True,\n",
    "                   transform=transform_train),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=False, transform=transform_test),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "detector_data_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_batch(bxs):\n",
    "    pil = transforms.ToPILImage()\n",
    "    return torch.squeeze(torch.stack([transform_test(pil(bx)) for bx in bxs]), dim=1)\n",
    "\n",
    "def get_batches(d, batch_size=32):\n",
    "    bx = []\n",
    "    by = []\n",
    "    tens = transforms.ToTensor()\n",
    "    for idx in range(0,len(d),batch_size):\n",
    "        bx_batch = torch.squeeze(torch.stack([tens(x[0]) for x in d[idx:idx+batch_size]]),dim=1)\n",
    "        bx.append(bx_batch)\n",
    "        by.append(torch.Tensor([x[1] for x in d[idx:idx+batch_size]]).type(torch.LongTensor))\n",
    "    \n",
    "    return bx, by\n",
    "\n",
    "def advs_p(p, bxs, bys, nrof_batches=None):\n",
    "    if nrof_batches is None:\n",
    "        nrof_batches = len(bxs)\n",
    "        \n",
    "    advs = []\n",
    "    for i in tqdm(range(len(bxs))):\n",
    "        if i >= nrof_batches:\n",
    "            break\n",
    "        \n",
    "        _, feats_reg = torch_model.gram_forward((bxs[i]*2 - 1).cuda())\n",
    "        advs_batch = p(torch_model, bxs[i].cuda(), bys[i].cuda())\n",
    "\n",
    "        advs.append(advs_batch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return advs\n",
    "\n",
    "def adversarial_acc(advs, bys):\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(advs)):\n",
    "        pipelined = pipeline_batch(advs[i].cpu())\n",
    "\n",
    "        x = pipelined.cuda()\n",
    "        y = bys[i].numpy()\n",
    "\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "    print(\"Adversarial Test Accuracy: \", correct/total)\n",
    "    \n",
    "def ds_grouped(bxs, bys):\n",
    "    ds = []\n",
    "    for i in range(len(bxs)):\n",
    "        pipelined = pipeline_batch(bxs[i].cpu())\n",
    "        for j in range(len(bxs[i])):\n",
    "            ds.append((pipelined[j], bys[i][j]))\n",
    "    return ds\n",
    "\n",
    "def adversarial_scores(detector, advs_batches, pbar = lambda x, total=None: x):\n",
    "    auroc = []\n",
    "    for batch in pbar(advs_batches):\n",
    "        auroc.append(detector.compute_ood_deviations_batch(batch*2 - 1)[\"AUROC\"])\n",
    "    \n",
    "    return np.mean(auroc)\n",
    "\n",
    "    \n",
    "    \n",
    "def model_accuracy():\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.numpy()\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "        \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Results </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(torch_model, data_train, data_test, 512, pbar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating L_Inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e322a79d70b7452191c81d5539a55b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adversarial Test Accuracy:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb4e1f16f164296b34ce6110a996fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9675182555379747"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "\n",
    "print(\"Calculating L_Inf\")\n",
    "xs, ys = get_batches(cifar10, batch_size=128)\n",
    "# pinf = PGD()\n",
    "pinf = adversary\n",
    "# pinf = PGD_margin().cuda()\n",
    "advs_inf = advs_p(pinf, xs, ys)\n",
    "\n",
    "adversarial_acc(advs_inf, ys)\n",
    "\n",
    "adversarial_scores(detector, advs_inf, pbar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    adv_logits, adv_feats = torch_model.gram_forward(advs_inf[0] * 2 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9640671874999998, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ood_deviations_advs(detector, adv_logits, adv_feats, ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advs_stats(detector, advs, ys):\n",
    "    preds = np.argmax(torch_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "random.shuffle(cifar10)\n",
    "xs, ys = get_batches(cifar10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(layer):\n",
    "    b, ch, h, w = layer.size()\n",
    "    features = layer.view(b, ch, w * h)\n",
    "    gram = torch.matmul(features, features.transpose(1, 2))\n",
    "    \n",
    "    return gram /(ch * h * w)\n",
    "\n",
    "def style_loss(lhs, rhs):\n",
    "    loss = 0.0\n",
    "    for i in range(len(lhs)):\n",
    "        loss += (gram_matrix(lhs[i]) - gram_matrix(rhs[i])).pow(2).sum()\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_vals(x, y):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "    logits_reg, feats_reg = torch_model.gram_forward(x*2 - 1)\n",
    "    \n",
    "#     adv_x = attacker_smart(torch_model, x, y, feats_reg)\n",
    "    adv_x = attacker_smart(torch_model, x, y)\n",
    "#     adv_x = attacker_naive(torch_model, x, y)\n",
    "    logits_adv, feats_adv = torch_model.gram_forward(adv_x * 2 - 1)\n",
    "    \n",
    "    x, y = x.cpu(), y.cpu()\n",
    "    adv_x = adv_x.cpu()\n",
    "    logits_adv = logits_adv.cpu()\n",
    "    \n",
    "    acc = (y==torch.max(logits_adv,dim=1)[1]).numpy().mean()\n",
    "    print(acc, detector.compute_ood_deviations_advs(logits_adv, feats_adv, y))\n",
    "    \n",
    "    return feats_reg, feats_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(x, y, margin=20, smart=True):\n",
    "    feats_reg, feats_adv = calc_vals(x, y)\n",
    "    \n",
    "    return style_loss(feats_reg, feats_adv).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007d433e60564fbba25106bbe91b152a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5477, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 0, Cent: 0.4385190010070801, Gram: 1.479410171508789, Total Loss: -1.040891170501709\n",
      "tensor(1.2740, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 1, Cent: 2.91074275970459, Gram: 0.20570087432861328, Total Loss: 2.7050418853759766\n",
      "tensor(7.7761, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 2, Cent: 5.7690324783325195, Gram: 6.707796096801758, Total Loss: -0.9387636184692383\n",
      "tensor(1.0648, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 3, Cent: 8.032779693603516, Gram: 0.0, Total Loss: 8.032779693603516\n",
      "tensor(6.5745, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 4, Cent: 10.797134399414062, Gram: 5.506235599517822, Total Loss: 5.29089879989624\n",
      "tensor(0.5986, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 5, Cent: 13.02049446105957, Gram: 0.0, Total Loss: 13.02049446105957\n",
      "tensor(2.0225, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 6, Cent: 16.273273468017578, Gram: 0.9541876316070557, Total Loss: 15.319086074829102\n",
      "tensor(0.9250, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 7, Cent: 16.472272872924805, Gram: 0.0, Total Loss: 16.472272872924805\n",
      "tensor(1.4969, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 8, Cent: 18.69086456298828, Gram: 0.4286835193634033, Total Loss: 18.26218032836914\n",
      "tensor(2.2551, device='cuda:2', grad_fn=<DivBackward0>) 32\n",
      "Step: 9, Cent: 18.116455078125, Gram: 1.186863660812378, Total Loss: 16.92959213256836\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-26829964aa97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-505677134331>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(x, y, margin, smart)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeats_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_vals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstyle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8607d6226aa0>\u001b[0m in \u001b[0;36mcalc_vals\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_adv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_ood_deviations_advs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeats_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/home/joelsimonoff/remote/gram-adversary-detection/model_training/utils/detector.py\u001b[0m in \u001b[0;36mcompute_ood_deviations_advs\u001b[0;34m(self, adv_logits, adv_feats, adv_ys)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0madv_deviations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madv_deviations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_dev_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfailed_adv_deviations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfailed_adv_deviations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_adv_dev_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mfailed_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    attacker_smart = PGD_Gram(gram_target=calc_gram_dev_target(), \n",
    "                              num_steps=10, \n",
    "                              epsilon=8./255, \n",
    "                              step_size=2./255, \n",
    "                              verbose=True)\n",
    "    \n",
    "#     attacker_smart = PGD_margin(style_weight = 0.0,\n",
    "#                                 epsilon=8./255, \n",
    "#                                 num_steps=10, \n",
    "#                                 step_size=2/255, \n",
    "#                                 verbose=True)\n",
    "#     attacker_naive = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255)\n",
    "    \n",
    "    for i, x in tqdm(enumerate(xs), total=len(xs)):\n",
    "        loss = process_batch(x, ys[i])\n",
    "        if True or i > 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def advs_stats(detector, advs_logits, advs_feats, ys):    \n",
    "\n",
    "def select_features(feat_list, idxs):\n",
    "    return [f[idxs] for f in feat_list]\n",
    "    \n",
    "def get_deviations(feat_list, mins,maxs):\n",
    "    if len(feat_list[0]) == 0:\n",
    "        return np.array([])\n",
    "    deviations = []\n",
    "    for L,feat_L in enumerate(feat_list):\n",
    "        \n",
    "        g_p = G_p(feat_L)\n",
    "\n",
    "        dev =  (F.relu(mins[L][0]-g_p)/torch.abs(mins[L][0]+10**-6)).sum(dim=1,keepdim=True)\n",
    "        dev +=  (F.relu(g_p-maxs[L][0])/torch.abs(maxs[L][0]+10**-6)).sum(dim=1,keepdim=True)\n",
    "\n",
    "        deviations.append(dev.cpu().numpy())\n",
    "            \n",
    "    deviations = np.concatenate(deviations, axis=1)\n",
    "    \n",
    "    return deviations\n",
    "    \n",
    "def compute_ood_deviations_advs(self, adv_logits, adv_feats, adv_ys):\n",
    "    confs = F.softmax(adv_logits,dim=1).cpu().numpy()\n",
    "    preds = np.argmax(confs,axis=1)\n",
    "\n",
    "    adv_deviations = None\n",
    "    failed_adv_deviations = None\n",
    "\n",
    "    for PRED in self.classes:\n",
    "        idxs = np.where(np.array(preds)==PRED)[0]\n",
    "        class_ys = np.array(adv_ys)[idxs]\n",
    "        idxs_failed = np.where(class_ys==PRED)[0]\n",
    "        \n",
    "        if len(idxs)==0:\n",
    "            continue\n",
    "\n",
    "        mins = self.mins[PRED]\n",
    "        maxs = self.maxs[PRED]\n",
    "        \n",
    "        adv_dev_class = get_deviations(select_features(adv_feats,idxs), mins=mins, maxs=maxs)\n",
    "        failed_adv_dev_class = get_deviations(select_features(adv_feats,idxs_failed), mins=mins, maxs=maxs)\n",
    "\n",
    "        if adv_deviations is None:\n",
    "            adv_deviations = adv_dev_class\n",
    "            failed_adv_deviations = failed_adv_dev_class\n",
    "            \n",
    "        else:\n",
    "            adv_deviations = np.concatenate([adv_deviations, adv_dev_class],axis=0)\n",
    "            failed_adv_deviations = np.concatenate([failed_adv_deviations, failed_adv_dev_class], axis=0)\n",
    "            \n",
    "    failed_results, results = 0,0\n",
    "    if len(failed_adv_deviations) != 0:\n",
    "        failed_results = detect(self.all_test_deviations, failed_adv_deviations)[\"AUROC\"]\n",
    "        \n",
    "    if len(adv_deviations) != 0:\n",
    "        results = detect(self.all_test_deviations, adv_deviations)[\"AUROC\"]\n",
    "    \n",
    "    return results, failed_results\n",
    "\n",
    "import utils.calculate_log as callog\n",
    "\n",
    "def calc_auroc(all_test_deviations,all_ood_deviations):\n",
    "    average_results = {}\n",
    "\n",
    "    test_deviations = all_test_deviations.sum(axis=1)\n",
    "    ood_deviations = all_ood_deviations.sum(axis=1)\n",
    "\n",
    "    results = callog.compute_metric(-test_deviations,-ood_deviations)\n",
    "\n",
    "    return results[\"AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers=[1]\n",
    "def cpu(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cpu()\n",
    "    return ob\n",
    "    \n",
    "def cuda(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cuda()\n",
    "    return ob\n",
    "def calc_gram_dev_target():\n",
    "    return detector.all_test_deviations.mean(axis=0).sum() \n",
    "\n",
    "def G_p_gpu(temp, p):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "# def G_p_gpu(ob, p):\n",
    "#     temp = ob\n",
    "    \n",
    "#     temp = temp**p\n",
    "#     temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "#     temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2) \n",
    "#     temp = (temp.sign()*torch.abs(temp)**(1/p)).reshape(temp.shape[0],-1)\n",
    "    \n",
    "#     return temp\n",
    "\n",
    "class PGD_Gram(nn.Module):\n",
    "    def __init__(self, epsilon=8/255, num_steps=10, step_size=2/255, grad_sign=True, \n",
    "                         mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5], nrof_classes=10, gram_target = 247, verbose=True):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "        \n",
    "        if mean is None:\n",
    "            self.mean = torch.FloatTensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.mean = torch.FloatTensor(mean).view(1,3,1,1).cuda()\n",
    "        if std is None:\n",
    "            self.std = torch.FloatTensor([0.2023, 0.1994, 0.2010]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.std = torch.FloatTensor(std).view(1,3,1,1).cuda()\n",
    "            \n",
    "        self.mns = [cuda(detector.mins[i]) for i in range(nrof_classes)]\n",
    "        self.mxs = [cuda(detector.maxs[i]) for i in range(nrof_classes)]\n",
    "        self.gram_target = gram_target * 0.85\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def get_deviation(self, feat_list, idx, mins, maxs, power=powers):\n",
    "        batch_deviations = []\n",
    "        for L,feat_L in enumerate(feat_list):\n",
    "            dev = 0\n",
    "            for p,P in enumerate(power):\n",
    "                g_p = G_p_gpu(feat_L,P)[idx]\n",
    "                \n",
    "                dev +=  (F.relu(mins[L][p]-g_p)/torch.abs(mins[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                dev +=  (F.relu(g_p-maxs[L][p])/torch.abs(maxs[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                \n",
    "                batch_deviations.append(dev)\n",
    "                \n",
    "        return batch_deviations\n",
    "        \n",
    "    def gram_loss(self, feats, logits):\n",
    "        confs = F.softmax(logits, dim=1)\n",
    "        _, indices = torch.max(confs, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(10):\n",
    "            idxs = indices == i\n",
    "\n",
    "            if idxs.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            batch_dev = self.get_deviation(feats, idxs, mins=self.mns[i], maxs=self.mxs[i])\n",
    "            batch_dev = torch.squeeze(torch.stack(batch_dev, dim=1))\n",
    "            \n",
    "            loss += batch_dev.sum()\n",
    "                            \n",
    "        return F.relu((loss/logits.shape[0]) - self.gram_target)\n",
    "    \n",
    "    def forward(self, model, bx, by):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats = model.gram_forward((adv_bx - self.mean)/self.std)\n",
    "                \n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                gram_loss =  self.gram_loss(feats, logits)\n",
    "                \n",
    "                loss = cent_loss - gram_loss\n",
    "                                \n",
    "            if self.verbose:\n",
    "                print(\"Step: {}, Cent: {}, Gram: {}, Total Loss: {}\".format(i, cent_loss, gram_loss, loss))\n",
    "            \n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "\n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_p(temp):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "class PGD_margin(nn.Module):\n",
    "    def __init__(self, epsilon=8./255, num_steps=10, step_size=2./255, style_weight = 100, grad_sign=True, verbose=False):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "        self.verbose = verbose\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "    def forward(self, model, bx, by, feats_reg):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "        \n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats_adv = model.gram_forward(adv_bx * 2 - 1)\n",
    "                s_loss = style_loss(feats_reg, feats_adv)\n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                \n",
    "                loss = cent_loss - self.style_weight * s_loss\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\"Step: {}, Cent: {}, Style Loss: {}, Total Loss: {}\".format(i, cent_loss, s_loss, loss))\n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "            \n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
