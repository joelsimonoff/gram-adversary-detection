{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.wrn import WideResNet\n",
    "import utils.attacks as attacks\n",
    "from utils.detector import Detector, new_gram_margin_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Trains a CIFAR Classifier',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--dataset', '-d', type=str, default='cifar10', choices=['cifar10', 'cifar100'],\n",
    "                    help='Choose between CIFAR-10, CIFAR-100.')\n",
    "parser.add_argument('--model', '-m', type=str, default='wrn',\n",
    "                    choices=['allconv', 'wrn'], help='Choose architecture.')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', '-e', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The initial learning rate.')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--test_bs', type=int, default=256)\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "# WRN Architecture\n",
    "parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n",
    "parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n",
    "parser.add_argument('--droprate', default=0.0, type=float, help='dropout probability')\n",
    "# Checkpoints\n",
    "parser.add_argument('--save', '-s', type=str, default='./snapshots/baseline', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--load', '-l', type=str, default='', help='Checkpoint path to resume / test.')\n",
    "parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--gpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "args = parser.parse_args([\"--save\", \"checkpoints/\", \"--gpu\", \"2\", \"--test_bs\", \"128\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'cifar10', 'test': False, 'save': 'checkpoints/', 'layers': 40, 'load': '', 'gpu': 2, 'learning_rate': 0.1, 'ngpu': 1, 'epochs': 100, 'test_bs': 128, 'prefetch': 2, 'model': 'wrn', 'droprate': 0.0, 'decay': 0.0005, 'batch_size': 128, 'widen_factor': 2, 'momentum': 0.9}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "print(state)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# # mean and standard deviation of channels of CIFAR-10 images\n",
    "# mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "# std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n",
    "                               trn.ToTensor()])\n",
    "test_transform = trn.Compose([trn.ToTensor()])\n",
    "\n",
    "train_data = dset.CIFAR10('~/datasets/cifarpy', train=True, transform=train_transform, download=True)\n",
    "test_data = dset.CIFAR10('~/datasets/cifarpy', train=False, transform=test_transform, download=True)\n",
    "num_classes = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=args.test_bs, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "normalize = trn.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "detector_data_transform = trn.Compose([trn.ToTensor(), normalize])\n",
    "\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if args.model == 'allconv':\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n",
    "    net.load_state_dict(torch.load(\"benchmark_ckpts/cifar10_reg_training_99.pt\"))\n",
    "    start_epoch = 80\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Restore model if desired\n",
    "if args.load != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(args.load, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "if args.ngpu > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n",
    "\n",
    "if args.ngpu > 0:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    net.cuda()\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args.epochs * len(train_loader),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args.learning_rate))\n",
    "\n",
    "\n",
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg, loss_gram_avg = 0.0, 0.0\n",
    "    i = 0\n",
    "    nan_counter = 0\n",
    "    \n",
    "#     mrgn_pwr = 1.6 if \"margin_log10\" not in state else state[\"margin_log10\"]\n",
    "#     if \"increase_margin\" in state and state[\"increase_margin\"]:\n",
    "#         mrgn_pwr += 0.1\n",
    "#         print(\"Increased Margin Power to:\", mrgn_pwr)\n",
    "\n",
    "    mrgn_pwr = 2\n",
    "    \n",
    "    for bx, by in tqdm(train_loader):\n",
    "        bx, by = bx.cuda(), by.cuda()\n",
    "\n",
    "        adv_bx = adversary(net, bx, by)\n",
    "                        \n",
    "        # forward\n",
    "        logits_reg, feats_reg = net.gram_forward(bx * 2 - 1)\n",
    "        logits_adv, feats_adv = net.gram_forward(adv_bx * 2 - 1)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_reg = F.cross_entropy(logits_reg, by)\n",
    "#         loss_adv = F.cross_entropy(logits_adv, by)\n",
    "        \n",
    "        loss_gram = new_gram_margin_loss(feats_reg, logits_reg, feats_adv, logits_adv, margin=20, golds=by).cuda()\n",
    "        loss = .99 * loss_reg + .01 * loss_gram\n",
    "                        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "        loss_gram_avg = loss_gram_avg * 0.8 + float(loss_gram) * 0.2\n",
    "    \n",
    "    state[\"increase_margin\"] = (loss_gram_avg < 0.1)\n",
    "    state[\"margin_log10\"] = mrgn_pwr\n",
    "    state['train_loss'] = loss_avg\n",
    "    state[\"gram_train_loss\"] = loss_gram_avg\n",
    "    \n",
    "    print(\"Train Loss:\", state[\"train_loss\"])\n",
    "    print(\"Margin Log10:\", state[\"margin_log10\"])\n",
    "    print(\"Train Gram: \", state[\"gram_train_loss\"])\n",
    "\n",
    "# test function\n",
    "def test(detector = None):\n",
    "    net.eval()\n",
    "    loss_avg, loss_reg_avg, loss_adv_avg, loss_gram_avg, auroc_avg = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss, loss_reg, loss_adv, loss_gram = 0.0, 0.0, 0.0, 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            adv_data = adversary(net, data, target)\n",
    "\n",
    "            # forward\n",
    "            output = net(data * 2 - 1)\n",
    "#             adv_output = net(adv_data * 2 - 1)\n",
    "            \n",
    "#             loss_reg = F.cross_entropy(output, target)\n",
    "#             loss_adv = F.cross_entropy(adv_output, target)\n",
    "            \n",
    "#             loss = loss_reg\n",
    "            \n",
    "            auroc = detector.compute_ood_deviations_batch(adv_data * 2 -1)\n",
    "            auroc = auroc[\"AUROC\"]\n",
    "            \n",
    "#             loss_gram = gram_margin_loss(feats_reg, feats_adv, margin=10e4)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).sum().item()\n",
    "\n",
    "            # test loss average\n",
    "#             loss_reg_avg += float(loss_reg)\n",
    "#             loss_adv_avg += float(loss_adv)\n",
    "#             loss_avg += float(loss)\n",
    "#             loss_gram_avg += float(loss_gram)\n",
    "            auroc_avg += auroc\n",
    "            \n",
    "            \n",
    "    state['test_loss_reg'] = loss_reg_avg / len(test_loader)\n",
    "    state['test_loss_adv'] = loss_adv_avg / len(test_loader)\n",
    "    state['test_loss'] = loss_avg / len(test_loader)\n",
    "    state['test_accuracy'] = correct / len(test_loader.dataset)\n",
    "    state['gram_loss'] = loss_gram_avg / len(test_loader)\n",
    "    state[\"gram_auroc\"] = auroc_avg / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a2aad259784c329925d28e7ff07601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.3055859199232964\n",
      "Margin Log10: 2\n",
      "Train Gram:  4.2919312746073865\n",
      "2. Initializing Detector\n",
      "3. Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63f53bb87834a3ea1d9b8d15acb4d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Time   767 | Train Loss 2.3056 | Test Loss 0.000 | Test Error 82.50 | Gram Auroc 0.12\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d6990606b24af6b0c6565f92a0f779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.259887910743908\n",
      "Margin Log10: 2\n",
      "Train Gram:  4.49161892864457\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89f660261fe4b57bdad1239ae94d8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9629108597270326\n",
      "Margin Log10: 2\n",
      "Train Gram:  3.5032244647471487\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750e781404324bc78bd752bb4ad8b138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9554834601403996\n",
      "Margin Log10: 2\n",
      "Train Gram:  0.7854481218842214\n",
      "2. Initializing Detector\n",
      "3. Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456fc9a5982748c9b7809faa66c49d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   4 | Time   817 | Train Loss 1.9555 | Test Loss 0.000 | Test Error 77.11 | Gram Auroc 0.11\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21831fc843f4cd28770151a4dc4f1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8947053469346968\n",
      "Margin Log10: 2\n",
      "Train Gram:  5.529057310731525\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d33db64580e4939a1c2ab074e0098c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.125703385367534\n",
      "Margin Log10: 2\n",
      "Train Gram:  10.059935192429364\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2442bb88510e404dbb01745ab52a74dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8539590012421543\n",
      "Margin Log10: 2\n",
      "Train Gram:  2.657821035022449\n",
      "2. Initializing Detector\n",
      "3. Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7351710c7041a195f4f56afd635918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   7 | Time   821 | Train Loss 1.8540 | Test Loss 0.000 | Test Error 67.23 | Gram Auroc 0.12\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd475c648f0340fa956d83dc2a499faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8346812493440061\n",
      "Margin Log10: 2\n",
      "Train Gram:  3.5314701590896043\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28140a158f6049339af7b39413d57e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0832029541712886\n",
      "Margin Log10: 2\n",
      "Train Gram:  0.09893203474411366\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df34108a61e44aceaba82bc3064ea6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8401405329508995\n",
      "Margin Log10: 2\n",
      "Train Gram:  0.5952706002502964\n",
      "2. Initializing Detector\n",
      "3. Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3951420fc3428092014476dc6d6bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  10 | Time   828 | Train Loss 1.8401 | Test Loss 0.000 | Test Error 68.31 | Gram Auroc 0.14\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded16f258826422983fd6051f6813793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make save directory\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.isdir(args.save):\n",
    "    raise Exception('%s is not a dir' % args.save)\n",
    "\n",
    "with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%),gram_auroc\\n')\n",
    "\n",
    "print('Beginning Training!\\n')\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "    \n",
    "    print(\"1. Training\")\n",
    "    train()\n",
    "    print(\"2. Initializing Detector\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        net.eval()\n",
    "        detector = Detector(net, data_train, data_test, args.test_bs, pbar=None)\n",
    "        print(\"3. Testing\")\n",
    "        try:\n",
    "            test(detector)\n",
    "        except Exception as e:\n",
    "            print(\"Failed test\")\n",
    "            print(e)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(net.state_dict(),\n",
    "                   os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                '_baseline_epoch_' + str(epoch) + '.pt'))\n",
    "        # Let us not waste space and delete the previous model\n",
    "        prev_path = os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                 '_baseline_epoch_' + str(epoch - 1) + '.pt')\n",
    "        if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "        # Show results\n",
    "\n",
    "        with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                          '_baseline_training_results.csv'), 'a') as f:\n",
    "            f.write('%03d,%05d,%0.6f,%0.5f,%0.2f,%0.2f\\n' % (\n",
    "                (epoch + 1),\n",
    "                time.time() - begin_epoch,\n",
    "                state['train_loss'],\n",
    "                state['test_loss'],\n",
    "                100 - 100. * state['test_accuracy'],\n",
    "                state[\"gram_auroc\"],\n",
    "            ))\n",
    "\n",
    "        # # print state with rounded decimals\n",
    "#         print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n",
    "\n",
    "        print('Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f} | Gram Auroc {5:.2f}'.format(\n",
    "            (epoch + 1),\n",
    "            int(time.time() - begin_epoch),\n",
    "            state['train_loss'],\n",
    "            state['test_loss'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "            state[\"gram_auroc\"])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
