{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>WRN: Cifar10</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable, grad\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "import utils.calculate_log as callog\n",
    "from utils.wrn import WideResNet\n",
    "\n",
    "from utils.detector import Detector, gram_margin_loss\n",
    "import utils.attacks as attacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "torch_model = WideResNet(depth=40, widen_factor=2, num_classes=10)\n",
    "\n",
    "# torch_model.load(path=\"benchmark_ckpts/cifar10_reg_training_99.pt\")\n",
    "# torch_model.load(path=\"benchmark_ckpts/cifar10_style_epoch_99.pt\")\n",
    "torch_model.load(path=\"checkpoints_style_fine_tuning/cifar10_wrn_baseline_epoch_2.pt\")\n",
    "torch_model.cuda()\n",
    "torch_model.params = list(torch_model.parameters())\n",
    "torch_model.eval()\n",
    "print(\"Done\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In-distribution Datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# mean = np.array([[125.3/255, 123.0/255, 113.9/255]]).T\n",
    "\n",
    "# std = np.array([[63.0/255, 62.1/255.0, 66.7/255.0]]).T\n",
    "# normalize = transforms.Normalize((125.3/255, 123.0/255, 113.9/255), (63.0/255, 62.1/255.0, 66.7/255.0))\n",
    "\n",
    "normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        \n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.CenterCrop(size=(32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=True, download=True,\n",
    "                   transform=transform_train),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=False, transform=transform_test),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "detector_data_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_batch(bxs):\n",
    "    pil = transforms.ToPILImage()\n",
    "    return torch.squeeze(torch.stack([transform_test(pil(bx)) for bx in bxs]), dim=1)\n",
    "\n",
    "def get_batches(d, batch_size=32):\n",
    "    bx = []\n",
    "    by = []\n",
    "    tens = transforms.ToTensor()\n",
    "    for idx in range(0,len(d),batch_size):\n",
    "        bx_batch = torch.squeeze(torch.stack([tens(x[0]) for x in d[idx:idx+batch_size]]),dim=1)\n",
    "        bx.append(bx_batch)\n",
    "        by.append(torch.Tensor([x[1] for x in d[idx:idx+batch_size]]).type(torch.LongTensor))\n",
    "    \n",
    "    return bx, by\n",
    "\n",
    "def advs_p(p, bxs, bys, nrof_batches=None):\n",
    "    if nrof_batches is None:\n",
    "        nrof_batches = len(bxs)\n",
    "        \n",
    "    advs = []\n",
    "    for i in tqdm(range(len(bxs))):\n",
    "        if i >= nrof_batches:\n",
    "            break\n",
    "        \n",
    "        _, feats_reg = torch_model.gram_forward((bxs[i]*2 - 1).cuda())\n",
    "        advs_batch = p(torch_model, bxs[i].cuda(), bys[i].cuda())\n",
    "\n",
    "        advs.append(advs_batch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return advs\n",
    "\n",
    "def adversarial_acc(advs, bys):\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(advs)):\n",
    "        pipelined = pipeline_batch(advs[i].cpu())\n",
    "\n",
    "        x = pipelined.cuda()\n",
    "        y = bys[i].numpy()\n",
    "\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "    print(\"Adversarial Test Accuracy: \", correct/total)\n",
    "    \n",
    "def ds_grouped(bxs, bys):\n",
    "    ds = []\n",
    "    for i in range(len(bxs)):\n",
    "        pipelined = pipeline_batch(bxs[i].cpu())\n",
    "        for j in range(len(bxs[i])):\n",
    "            ds.append((pipelined[j], bys[i][j]))\n",
    "    return ds\n",
    "\n",
    "def adversarial_scores(detector, advs_batches, pbar = lambda x, total=None: x):\n",
    "    auroc = []\n",
    "    for batch in pbar(advs_batches):\n",
    "        auroc.append(detector.compute_ood_deviations_batch(batch*2 - 1)[\"AUROC\"])\n",
    "    \n",
    "    return np.mean(auroc)\n",
    "    \n",
    "    \n",
    "def model_accuracy():\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.numpy()\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "        \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Results </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8818"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(torch_model, data_train, data_test, 512, pbar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating L_Inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c5bda6df794e4081115465de24d974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adversarial Test Accuracy:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc083fedf547452586100c89ca4c1420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9844936412183541"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "\n",
    "print(\"Calculating L_Inf\")\n",
    "xs, ys = get_batches(cifar10, batch_size=128)\n",
    "# pinf = PGD()\n",
    "pinf = adversary\n",
    "# pinf = PGD_margin().cuda()\n",
    "advs_inf = advs_p(pinf, xs, ys)\n",
    "\n",
    "adversarial_acc(advs_inf, ys)\n",
    "\n",
    "adversarial_scores(detector, advs_inf, pbar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "xs, ys = get_batches(cifar10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(layer):\n",
    "    b, ch, h, w = layer.size()\n",
    "    features = layer.view(b, ch, w * h)\n",
    "    gram = torch.matmul(features, features.transpose(1, 2))\n",
    "    \n",
    "    return gram /(ch * h * w)\n",
    "\n",
    "def style_loss(lhs, rhs):\n",
    "    loss = 0.0\n",
    "    for i in range(len(lhs)):\n",
    "        loss += (gram_matrix(lhs[i]) - gram_matrix(rhs[i])).pow(2).sum()\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_vals(x, y):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "    logits_reg, feats_reg = torch_model.gram_forward(x*2 - 1)\n",
    "    \n",
    "#     adv_x = attacker_smart(torch_model, x, y, feats_reg)\n",
    "    adv_x = attacker_smart(torch_model, x, y)\n",
    "    \n",
    "    logits_adv, feats_adv = torch_model.gram_forward(adv_x * 2 - 1)\n",
    "    \n",
    "    x, y = x.cpu(), y.cpu()\n",
    "    adv_x = adv_x.cpu()\n",
    "    logits_adv = logits_adv.cpu()\n",
    "    \n",
    "    acc = (y==torch.max(logits_adv,dim=1)[1]).numpy().mean()\n",
    "    print(acc, detector.compute_ood_deviations_batch(adv_x * 2 -1)[\"AUROC\"])\n",
    "    \n",
    "    return feats_reg, feats_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(x, y, margin=20, smart=True):\n",
    "    feats_reg, feats_adv = calc_vals(x, y)\n",
    "    \n",
    "    return style_loss(feats_reg, feats_adv).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d1e971458546718646941c8fd70c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    attacker_smart = PGD_Gram(gram_target=calc_gram_dev_target(), verbose=False)\n",
    "#     attacker_smart = PGD_margin(epsilon=8./255, num_steps=10, step_size=2/255)\n",
    "#     attacker_naive = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255)\n",
    "    \n",
    "    for i, x in tqdm(enumerate(xs), total=len(xs)):\n",
    "        loss = process_batch(x, ys[i])\n",
    "        if i > 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers=[1]\n",
    "def cpu(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cpu()\n",
    "    return ob\n",
    "    \n",
    "def cuda(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cuda()\n",
    "    return ob\n",
    "def calc_gram_dev_target():\n",
    "    return detector.all_test_deviations.mean(axis=0).sum() \n",
    "\n",
    "def G_p_gpu(temp, p):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "# def G_p_gpu(ob, p):\n",
    "#     temp = ob\n",
    "    \n",
    "#     temp = temp**p\n",
    "#     temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "#     temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2) \n",
    "#     temp = (temp.sign()*torch.abs(temp)**(1/p)).reshape(temp.shape[0],-1)\n",
    "    \n",
    "#     return temp\n",
    "\n",
    "class PGD_Gram(nn.Module):\n",
    "    def __init__(self, epsilon=8/255, num_steps=10, step_size=2/255, grad_sign=True, \n",
    "                         mean = None, std = None, nrof_classes=10, gram_target = 247, verbose=True):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "        \n",
    "        if mean is None:\n",
    "            self.mean = torch.FloatTensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.mean = torch.FloatTensor(mean).view(1,3,1,1).cuda()\n",
    "        if std is None:\n",
    "            self.std = torch.FloatTensor([0.2023, 0.1994, 0.2010]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.std = torch.FloatTensor(std).view(1,3,1,1).cuda()\n",
    "            \n",
    "        self.mns = [cuda(detector.mins[i]) for i in range(nrof_classes)]\n",
    "        self.mxs = [cuda(detector.maxs[i]) for i in range(nrof_classes)]\n",
    "        self.gram_target = gram_target * 0.85\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def get_deviation(self, feat_list, idx, mins, maxs, power=powers):\n",
    "        batch_deviations = []\n",
    "        for L,feat_L in enumerate(feat_list):\n",
    "            dev = 0\n",
    "            for p,P in enumerate(power):\n",
    "                g_p = G_p_gpu(feat_L,P)[idx]\n",
    "                \n",
    "                dev +=  (F.relu(mins[L][p]-g_p)/torch.abs(mins[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                dev +=  (F.relu(g_p-maxs[L][p])/torch.abs(maxs[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                \n",
    "                batch_deviations.append(dev)\n",
    "                \n",
    "        return batch_deviations\n",
    "        \n",
    "    def gram_loss(self, feats, logits):\n",
    "        confs = F.softmax(logits, dim=1)\n",
    "        _, indices = torch.max(confs, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(10):\n",
    "            idxs = indices == i\n",
    "\n",
    "            if idxs.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            batch_dev = self.get_deviation(feats, idxs, mins=self.mns[i], maxs=self.mxs[i])\n",
    "            batch_dev = torch.squeeze(torch.stack(batch_dev, dim=1))\n",
    "            \n",
    "            loss += batch_dev.sum()\n",
    "                \n",
    "        return F.relu(loss - logits.shape[0] * self.gram_target)\n",
    "    \n",
    "    def forward(self, model, bx, by):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats = model.gram_forward((adv_bx - self.mean)/self.std)\n",
    "                \n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                gram_loss = self.gram_loss(feats, logits)\n",
    "                \n",
    "                loss = cent_loss - gram_loss\n",
    "                                \n",
    "            if self.verbose:\n",
    "                print(\"Step: {}, Cent: {}, Gram: {}, Total Loss: {}\".format(i, cent_loss, gram_loss, loss))\n",
    "            \n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "\n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_p(temp):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "class PGD_margin(nn.Module):\n",
    "    def __init__(self, epsilon=8./255, num_steps=10, step_size=2./255, grad_sign=True):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "\n",
    "    def forward(self, model, bx, by, feats_reg):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "        \n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats_adv = model.gram_forward(adv_bx * 2 - 1)\n",
    "                s_loss = style_loss(feats_reg, feats_adv)\n",
    "                \n",
    "                loss = F.cross_entropy(logits, by, reduction='sum') - s_loss\n",
    "                print(loss, s_loss)\n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "            \n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found an interesting flaw in the margin loss that I think is the root of a lot of my struggles. With the margin loss we are encouraging a margin between an example and its corresponding adversary example. This was barely working and very finnicky. After lots of digging, I discovered a better version of the margin. \n",
    "\n",
    "Here is a basic walk through of the adversarial detector:\n",
    "See which class an example is predicting as.\n",
    "See if that class violates adversarial deviation bounds.\n",
    "Classify as adversarial or not\n",
    "\n",
    "Here is how the margin loss was working:\n",
    "Take in a regular example and create a corresponding adversarial example.\n",
    "Enforce margin in gram deviations between the two.\n",
    "\n",
    "Here is my new proposed version:\n",
    "Take in a batch of regular examples and create a corresponding batch of adv examples.\n",
    "Pair up regular examples with adversarial examples that predict to the same class. \n",
    "Enforce margin within pairs.\n",
    "\n",
    "Here is why I think this will work better:\n",
    "When adversarially perturbing an example we are changing the class it is predicting to. When detecting we classify it based on the known gram deviations of that class. My hypothesis is that adversarial perturbing an image doesn’t change its gram values that much, rather each class has its own separate gram range. \n",
    "\n",
    "Tests I’ve done so far:\n",
    "\n",
    "Confirmed that adversarial perturbing an input using PGD doesn’t change the gram values that much it obviously does a little.\n",
    "When generating adversarial examples that beat the detector, the new_margin loss is high when the detector AUROC approaches 50 and is 0.0 as it approaches .97."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
