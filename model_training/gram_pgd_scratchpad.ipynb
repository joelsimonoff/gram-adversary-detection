{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>WRN: Cifar10</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable, grad\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "import pandas as pd\n",
    "import utils.calculate_log as callog\n",
    "from utils.wrn import WideResNet\n",
    "\n",
    "from utils.detector import Detector, gram_margin_loss\n",
    "import utils.attacks as attacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "torch_model = WideResNet(depth=40, widen_factor=2, num_classes=10)\n",
    "\n",
    "torch_model.load(path=\"benchmark_ckpts/cifar10_reg_training_99.pt\")\n",
    "# torch_model.load(path=\"benchmark_ckpts/cifar10_style_epoch_99.pt\")\n",
    "# torch_model.load(path=\"checkpoints_style_fine_tuning/cifar10_wrn_baseline_epoch_2.pt\")\n",
    "torch_model.cuda()\n",
    "torch_model.params = list(torch_model.parameters())\n",
    "torch_model.eval()\n",
    "print(\"Done\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In-distribution Datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# mean = np.array([[125.3/255, 123.0/255, 113.9/255]]).T\n",
    "\n",
    "# std = np.array([[63.0/255, 62.1/255.0, 66.7/255.0]]).T\n",
    "# normalize = transforms.Normalize((125.3/255, 123.0/255, 113.9/255), (63.0/255, 62.1/255.0, 66.7/255.0))\n",
    "\n",
    "normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        \n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.CenterCrop(size=(32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=True, download=True,\n",
    "                   transform=transform_train),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('~/datasets/cifarpy', train=False, transform=transform_test),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "detector_data_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_batch(bxs):\n",
    "    pil = transforms.ToPILImage()\n",
    "    return torch.squeeze(torch.stack([transform_test(pil(bx)) for bx in bxs]), dim=1)\n",
    "\n",
    "def get_batches(d, batch_size=32):\n",
    "    bx = []\n",
    "    by = []\n",
    "    tens = transforms.ToTensor()\n",
    "    for idx in range(0,len(d),batch_size):\n",
    "        bx_batch = torch.squeeze(torch.stack([tens(x[0]) for x in d[idx:idx+batch_size]]),dim=1)\n",
    "        bx.append(bx_batch)\n",
    "        by.append(torch.Tensor([x[1] for x in d[idx:idx+batch_size]]).type(torch.LongTensor))\n",
    "    \n",
    "    return bx, by\n",
    "\n",
    "def advs_p(p, bxs, bys, nrof_batches=None):\n",
    "    if nrof_batches is None:\n",
    "        nrof_batches = len(bxs)\n",
    "        \n",
    "    advs = []\n",
    "    for i in tqdm(range(len(bxs))):\n",
    "        if i >= nrof_batches:\n",
    "            break\n",
    "        \n",
    "        _, feats_reg = torch_model.gram_forward((bxs[i]*2 - 1).cuda())\n",
    "        advs_batch = p(torch_model, bxs[i].cuda(), bys[i].cuda())\n",
    "\n",
    "        advs.append(advs_batch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return advs\n",
    "\n",
    "def adversarial_acc(advs, bys):\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(advs)):\n",
    "        pipelined = pipeline_batch(advs[i].cpu())\n",
    "\n",
    "        x = pipelined.cuda()\n",
    "        y = bys[i].numpy()\n",
    "\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "    print(\"Adversarial Test Accuracy: \", correct/total)\n",
    "    \n",
    "def ds_grouped(bxs, bys):\n",
    "    ds = []\n",
    "    for i in range(len(bxs)):\n",
    "        pipelined = pipeline_batch(bxs[i].cpu())\n",
    "        for j in range(len(bxs[i])):\n",
    "            ds.append((pipelined[j], bys[i][j]))\n",
    "    return ds\n",
    "\n",
    "def adversarial_scores(detector, advs_batches, pbar = lambda x, total=None: x):\n",
    "    auroc = []\n",
    "    for batch in pbar(advs_batches):\n",
    "        auroc.append(detector.compute_ood_deviations_batch(batch*2 - 1)[\"AUROC\"])\n",
    "    \n",
    "    return np.mean(auroc)\n",
    "\n",
    "    \n",
    "    \n",
    "def model_accuracy():\n",
    "    torch_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.numpy()\n",
    "        correct += (y==np.argmax(torch_model(x).detach().cpu().numpy(),axis=1)).sum()\n",
    "        total += y.shape[0]\n",
    "        \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Results </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(torch_model, data_train, data_test, 512, pbar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating L_Inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e322a79d70b7452191c81d5539a55b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adversarial Test Accuracy:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb4e1f16f164296b34ce6110a996fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9675182555379747"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "\n",
    "print(\"Calculating L_Inf\")\n",
    "xs, ys = get_batches(cifar10, batch_size=128)\n",
    "# pinf = PGD()\n",
    "pinf = adversary\n",
    "# pinf = PGD_margin().cuda()\n",
    "advs_inf = advs_p(pinf, xs, ys)\n",
    "\n",
    "adversarial_acc(advs_inf, ys)\n",
    "\n",
    "adversarial_scores(detector, advs_inf, pbar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    adv_logits, adv_feats = torch_model.gram_forward(advs_inf[0] * 2 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9640671874999998, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ood_deviations_advs(detector, adv_logits, adv_feats, ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advs_stats(detector, advs, ys):\n",
    "    preds = np.argmax(torch_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = list(datasets.CIFAR10('~/datasets/cifarpy', train=False))\n",
    "random.shuffle(cifar10)\n",
    "xs, ys = get_batches(cifar10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(layer):\n",
    "    b, ch, h, w = layer.size()\n",
    "    features = layer.view(b, ch, w * h)\n",
    "    gram = torch.matmul(features, features.transpose(1, 2))\n",
    "    \n",
    "    return gram /(ch * h * w)\n",
    "\n",
    "def style_loss(lhs, rhs):\n",
    "    loss = 0.0\n",
    "    for i in range(len(lhs)):\n",
    "        loss += (gram_matrix(lhs[i]) - gram_matrix(rhs[i])).pow(2).sum()\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_vals(x, y):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    \n",
    "    logits_reg, feats_reg = torch_model.gram_forward(x*2 - 1)\n",
    "    \n",
    "#     adv_x = attacker_smart(torch_model, x, y, feats_reg)\n",
    "    adv_x = attacker_smart(torch_model, x, y)\n",
    "#     adv_x = attacker_naive(torch_model, x, y)\n",
    "    logits_adv, feats_adv = torch_model.gram_forward(adv_x * 2 - 1)\n",
    "    \n",
    "    x, y = x.cpu(), y.cpu()\n",
    "    adv_x = adv_x.cpu()\n",
    "    logits_adv = logits_adv.cpu()\n",
    "    \n",
    "    acc = (y==torch.max(logits_adv,dim=1)[1]).numpy().mean()\n",
    "    auroc, auroc_failed = detector.compute_auroc_advs(logits_adv, feats_adv, y)\n",
    "    \n",
    "    return feats_reg, feats_adv, auroc, auroc_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(x, y, margin=20, smart=True):\n",
    "    feats_reg, feats_adv, auroc, auroc_failed = calc_vals(x, y)\n",
    "    \n",
    "#     return style_loss(feats_reg, feats_adv).cpu()\n",
    "    return auroc, auroc_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9d200932314cf2a51bc849af5a9d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    attacker_smart = PGD_Gram(gram_target=calc_gram_dev_target(), \n",
    "                              num_steps=10, \n",
    "                              epsilon=8./255, \n",
    "                              step_size=2./255, \n",
    "                              verbose=False)\n",
    "    \n",
    "#     attacker_smart = PGD_margin(style_weight = 0.0,\n",
    "#                                 epsilon=8./255, \n",
    "#                                 num_steps=10, \n",
    "#                                 step_size=2/255, \n",
    "#                                 verbose=True)\n",
    "#     attacker_naive = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255)\n",
    "    \n",
    "    auroc, auroc_failed = [], []\n",
    "    for i, x in tqdm(enumerate(xs), total=len(xs)):\n",
    "        if i % 3 != 0:\n",
    "            continue\n",
    "        a, a_f = process_batch(x, ys[i])\n",
    "        auroc.append(a)\n",
    "        auroc_failed.append(a_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.689618254289906, 0.5497135921075128)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(auroc).mean(), np.array(auroc_failed).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gram Loss Detector:\n",
    "\n",
    "- Auroc: `(0.689618254289906, 0.5497135921075128)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers=[1]\n",
    "def cpu(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cpu()\n",
    "    return ob\n",
    "    \n",
    "def cuda(ob):\n",
    "    for i in range(len(ob)):\n",
    "        for j in range(len(ob[i])):\n",
    "            ob[i][j] = ob[i][j].cuda()\n",
    "    return ob\n",
    "def calc_gram_dev_target():\n",
    "    return detector.all_test_deviations.mean(axis=0).sum() \n",
    "\n",
    "def G_p_gpu(temp, p):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "# def G_p_gpu(ob, p):\n",
    "#     temp = ob\n",
    "    \n",
    "#     temp = temp**p\n",
    "#     temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "#     temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2) \n",
    "#     temp = (temp.sign()*torch.abs(temp)**(1/p)).reshape(temp.shape[0],-1)\n",
    "    \n",
    "#     return temp\n",
    "\n",
    "class PGD_Gram(nn.Module):\n",
    "    def __init__(self, epsilon=8/255, num_steps=10, step_size=2/255, grad_sign=True, \n",
    "                         mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5], nrof_classes=10, gram_target = 247, verbose=True):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "        \n",
    "        if mean is None:\n",
    "            self.mean = torch.FloatTensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.mean = torch.FloatTensor(mean).view(1,3,1,1).cuda()\n",
    "        if std is None:\n",
    "            self.std = torch.FloatTensor([0.2023, 0.1994, 0.2010]).view(1,3,1,1).cuda()\n",
    "        else:\n",
    "            self.std = torch.FloatTensor(std).view(1,3,1,1).cuda()\n",
    "            \n",
    "        self.mns = [cuda(detector.mins[i]) for i in range(nrof_classes)]\n",
    "        self.mxs = [cuda(detector.maxs[i]) for i in range(nrof_classes)]\n",
    "        self.gram_target = gram_target * 0.85\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def get_deviation(self, feat_list, idx, mins, maxs, power=powers):\n",
    "        batch_deviations = []\n",
    "        for L,feat_L in enumerate(feat_list):\n",
    "            dev = 0\n",
    "            for p,P in enumerate(power):\n",
    "                g_p = G_p_gpu(feat_L,P)[idx]\n",
    "                \n",
    "                dev +=  (F.relu(mins[L][p]-g_p)/torch.abs(mins[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                dev +=  (F.relu(g_p-maxs[L][p])/torch.abs(maxs[L][p]+10**-6)).sum(dim=1,keepdim=True)\n",
    "                \n",
    "                batch_deviations.append(dev)\n",
    "                \n",
    "        return batch_deviations\n",
    "        \n",
    "    def gram_loss(self, feats, logits):\n",
    "        confs = F.softmax(logits, dim=1)\n",
    "        _, indices = torch.max(confs, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(10):\n",
    "            idxs = indices == i\n",
    "\n",
    "            if idxs.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            batch_dev = self.get_deviation(feats, idxs, mins=self.mns[i], maxs=self.mxs[i])\n",
    "            batch_dev = torch.squeeze(torch.stack(batch_dev, dim=1))\n",
    "            \n",
    "            loss += batch_dev.sum()\n",
    "                            \n",
    "        return F.relu((loss/logits.shape[0]) - self.gram_target)\n",
    "    \n",
    "    def forward(self, model, bx, by):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats = model.gram_forward((adv_bx - self.mean)/self.std)\n",
    "                \n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                gram_loss =  self.gram_loss(feats, logits)\n",
    "                \n",
    "                loss = cent_loss - gram_loss\n",
    "                                \n",
    "            if self.verbose:\n",
    "                print(\"Step: {}, Cent: {}, Gram: {}, Total Loss: {}\".format(i, cent_loss, gram_loss, loss))\n",
    "            \n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "\n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_p(temp):\n",
    "    temp = temp.reshape(temp.shape[0],temp.shape[1],-1)\n",
    "    temp = ((torch.matmul(temp,temp.transpose(dim0=2,dim1=1)))).sum(dim=2)\n",
    "    return temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "class PGD_margin(nn.Module):\n",
    "    def __init__(self, epsilon=8./255, num_steps=10, step_size=2./255, style_weight = 100, grad_sign=True, verbose=False):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.grad_sign = grad_sign\n",
    "        self.verbose = verbose\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "    def forward(self, model, bx, by, feats_reg):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "        \n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits, feats_adv = model.gram_forward(adv_bx * 2 - 1)\n",
    "                s_loss = style_loss(feats_reg, feats_adv)\n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                \n",
    "                loss = cent_loss - self.style_weight * s_loss\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\"Step: {}, Cent: {}, Style Loss: {}, Total Loss: {}\".format(i, cent_loss, s_loss, loss))\n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon).clamp(0, 1)\n",
    "            \n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def advs_stats(detector, advs_logits, advs_feats, ys):    \n",
    "\n",
    "def select_features(feat_list, idxs):\n",
    "    return [f[idxs] for f in feat_list]\n",
    "    \n",
    "def get_deviations(feat_list, mins,maxs):\n",
    "    if len(feat_list[0]) == 0:\n",
    "        return np.array([])\n",
    "    deviations = []\n",
    "    for L,feat_L in enumerate(feat_list):\n",
    "        \n",
    "        g_p = G_p(feat_L)\n",
    "\n",
    "        dev =  (F.relu(mins[L][0]-g_p)/torch.abs(mins[L][0]+10**-6)).sum(dim=1,keepdim=True)\n",
    "        dev +=  (F.relu(g_p-maxs[L][0])/torch.abs(maxs[L][0]+10**-6)).sum(dim=1,keepdim=True)\n",
    "\n",
    "        deviations.append(dev.cpu().numpy())\n",
    "            \n",
    "    deviations = np.concatenate(deviations, axis=1)\n",
    "    \n",
    "    return deviations\n",
    "    \n",
    "def compute_ood_deviations_advs(self, adv_logits, adv_feats, adv_ys):\n",
    "    confs = F.softmax(adv_logits,dim=1).cpu().numpy()\n",
    "    preds = np.argmax(confs,axis=1)\n",
    "\n",
    "    adv_deviations = None\n",
    "    failed_adv_deviations = None\n",
    "\n",
    "    for PRED in self.classes:\n",
    "        idxs = np.where(np.array(preds)==PRED)[0]\n",
    "        class_ys = np.array(adv_ys)[idxs]\n",
    "        idxs_failed = np.where(class_ys==PRED)[0]\n",
    "        \n",
    "        if len(idxs)==0:\n",
    "            continue\n",
    "\n",
    "        mins = self.mins[PRED]\n",
    "        maxs = self.maxs[PRED]\n",
    "        \n",
    "        adv_dev_class = get_deviations(select_features(adv_feats,idxs), mins=mins, maxs=maxs)\n",
    "        failed_adv_dev_class = get_deviations(select_features(adv_feats,idxs_failed), mins=mins, maxs=maxs)\n",
    "\n",
    "        if adv_deviations is None:\n",
    "            adv_deviations = adv_dev_class\n",
    "            failed_adv_deviations = failed_adv_dev_class\n",
    "            \n",
    "        else:\n",
    "            adv_deviations = np.concatenate([adv_deviations, adv_dev_class],axis=0)\n",
    "            failed_adv_deviations = np.concatenate([failed_adv_deviations, failed_adv_dev_class], axis=0)\n",
    "            \n",
    "    failed_results, results = 0,0\n",
    "    if len(failed_adv_deviations) != 0:\n",
    "        failed_results = detect(self.all_test_deviations, failed_adv_deviations)[\"AUROC\"]\n",
    "        \n",
    "    if len(adv_deviations) != 0:\n",
    "        results = detect(self.all_test_deviations, adv_deviations)[\"AUROC\"]\n",
    "    \n",
    "    return results, failed_results\n",
    "\n",
    "import utils.calculate_log as callog\n",
    "\n",
    "def calc_auroc(all_test_deviations,all_ood_deviations):\n",
    "    average_results = {}\n",
    "\n",
    "    test_deviations = all_test_deviations.sum(axis=1)\n",
    "    ood_deviations = all_ood_deviations.sum(axis=1)\n",
    "\n",
    "    results = callog.compute_metric(-test_deviations,-ood_deviations)\n",
    "\n",
    "    return results[\"AUROC\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
