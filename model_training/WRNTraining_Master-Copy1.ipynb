{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.wrn import WideResNet\n",
    "import utils.attacks as attacks\n",
    "from utils.detector import Detector, style_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Trains a CIFAR Classifier',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--dataset', '-d', type=str, default='cifar10', choices=['cifar10', 'cifar100'],\n",
    "                    help='Choose between CIFAR-10, CIFAR-100.')\n",
    "parser.add_argument('--model', '-m', type=str, default='wrn',\n",
    "                    choices=['allconv', 'wrn'], help='Choose architecture.')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', '-e', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The initial learning rate.')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--test_bs', type=int, default=256)\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "# WRN Architecture\n",
    "parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n",
    "parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n",
    "parser.add_argument('--droprate', default=0.0, type=float, help='dropout probability')\n",
    "# Checkpoints\n",
    "parser.add_argument('--save', '-s', type=str, default='./snapshots/baseline', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--load', '-l', type=str, default='', help='Checkpoint path to resume / test.')\n",
    "parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--gpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "args = parser.parse_args([\"--save\", \"checkpoints/\", \"--gpu\", \"3\", \"--test_bs\", \"128\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prefetch': 2, 'epochs': 100, 'load': '', 'test': False, 'decay': 0.0005, 'batch_size': 128, 'model': 'wrn', 'layers': 40, 'save': 'checkpoints/', 'ngpu': 1, 'droprate': 0.0, 'dataset': 'cifar10', 'learning_rate': 0.1, 'test_bs': 128, 'gpu': 3, 'widen_factor': 2, 'momentum': 0.9}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "print(state)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# # mean and standard deviation of channels of CIFAR-10 images\n",
    "# mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "# std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n",
    "                               trn.ToTensor()])\n",
    "test_transform = trn.Compose([trn.ToTensor()])\n",
    "\n",
    "train_data = dset.CIFAR10('~/datasets/cifarpy', train=True, transform=train_transform, download=True)\n",
    "test_data = dset.CIFAR10('~/datasets/cifarpy', train=False, transform=test_transform, download=True)\n",
    "num_classes = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=args.test_bs, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "normalize = trn.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "detector_data_transform = trn.Compose([trn.ToTensor(), normalize])\n",
    "\n",
    "data_train = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=True, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n",
    "\n",
    "data_test = list(torch.utils.data.DataLoader(\n",
    "        dset.CIFAR10('~/datasets/cifarpy', \n",
    "                     train=False, \n",
    "                     transform=detector_data_transform, \n",
    "                     download=True),\n",
    "        batch_size=1, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if args.model == 'allconv':\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n",
    "    net.load_state_dict(torch.load(\"benchmark_ckpts/cifar10_reg_training_99.pt\"))\n",
    "    start_epoch = 80\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Restore model if desired\n",
    "if args.load != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(args.load, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "if args.ngpu > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n",
    "\n",
    "if args.ngpu > 0:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    net.cuda()\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args.epochs * len(train_loader),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args.learning_rate))\n",
    "\n",
    "\n",
    "adversary = attacks.PGD(epsilon=8./255, num_steps=10, step_size=2./255).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg, loss_gram_avg = 0.0, 0.0\n",
    "    \n",
    "    for bx, by in tqdm(train_loader):\n",
    "        bx, by = bx.cuda(), by.cuda()\n",
    "\n",
    "        adv_bx = adversary(net, bx, by)\n",
    "                        \n",
    "        # forward\n",
    "        logits_reg, feats_reg = net.gram_forward(bx * 2 - 1)\n",
    "        logits_adv, feats_adv = net.gram_forward(adv_bx * 2 - 1)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_reg = F.cross_entropy(logits_reg, by)\n",
    "#         loss_adv = F.cross_entropy(logits_adv, by)\n",
    "        \n",
    "        loss_gram = style_loss(feats_reg, feats_adv).cuda()\n",
    "        loss = loss_reg + 10**-2 * F.relu(10**3 - loss_gram)\n",
    "                        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "        loss_gram_avg = loss_gram_avg * 0.8 + float(loss_gram) * 0.2\n",
    "    \n",
    "    state['train_loss'] = loss_avg\n",
    "    state[\"gram_train_loss\"] = loss_gram_avg\n",
    "    \n",
    "    print(\"Train Loss:\", state[\"train_loss\"])\n",
    "    print(\"Train Gram: \", state[\"gram_train_loss\"])\n",
    "\n",
    "# test function\n",
    "def test(detector = None):\n",
    "    net.eval()\n",
    "    loss_avg, loss_reg_avg, loss_adv_avg, loss_gram_avg, auroc_avg = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss, loss_reg, loss_adv, loss_gram = 0.0, 0.0, 0.0, 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            adv_data = adversary(net, data, target)\n",
    "\n",
    "            # forward\n",
    "            output = net(data * 2 - 1)\n",
    "#             adv_output = net(adv_data * 2 - 1)\n",
    "            \n",
    "#             loss_reg = F.cross_entropy(output, target)\n",
    "#             loss_adv = F.cross_entropy(adv_output, target)\n",
    "            \n",
    "#             loss = loss_reg\n",
    "            \n",
    "            auroc = detector.compute_ood_deviations_batch(adv_data * 2 -1)\n",
    "            auroc = auroc[\"AUROC\"]\n",
    "            \n",
    "#             loss_gram = gram_margin_loss(feats_reg, feats_adv, margin=10e4)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).sum().item()\n",
    "\n",
    "            # test loss average\n",
    "#             loss_reg_avg += float(loss_reg)\n",
    "#             loss_adv_avg += float(loss_adv)\n",
    "#             loss_avg += float(loss)\n",
    "#             loss_gram_avg += float(loss_gram)\n",
    "            auroc_avg += auroc\n",
    "            \n",
    "            \n",
    "    state['test_loss_reg'] = loss_reg_avg / len(test_loader)\n",
    "    state['test_loss_adv'] = loss_adv_avg / len(test_loader)\n",
    "    state['test_loss'] = loss_avg / len(test_loader)\n",
    "    state['test_accuracy'] = correct / len(test_loader.dataset)\n",
    "    state['gram_loss'] = loss_gram_avg / len(test_loader)\n",
    "    state[\"gram_auroc\"] = auroc_avg / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089320e1879e410cb9b2069984550e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8129208917852586\n",
      "Train Gram:  10912030430.609653\n",
      "2. Initializing Detector\n",
      "3. Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94352d1b4f814471ace1351e2c982fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1 | Time   380 | Train Loss 1.8129 | Test Loss 0.000 | Test Error 74.66 | Gram Auroc 0.41\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320b79a62ec1467a8c3b69ab412fb9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.401013922267157\n",
      "Train Gram:  1241329233.7979374\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a1a1d0aaf4b79aeaceca5db2db46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0054693808757253\n",
      "Train Gram:  219770142.46359578\n",
      "2. Initializing Detector\n",
      "1. Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e316e294364fea955f1cadd998dfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make save directory\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.isdir(args.save):\n",
    "    raise Exception('%s is not a dir' % args.save)\n",
    "\n",
    "with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                  '_baseline_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%),gram_auroc\\n')\n",
    "\n",
    "print('Beginning Training!\\n')\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "    \n",
    "    print(\"1. Training\")\n",
    "    train()\n",
    "    print(\"2. Initializing Detector\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        net.eval()\n",
    "        detector = Detector(net, data_train, data_test, args.test_bs, pbar=None)\n",
    "        print(\"3. Testing\")\n",
    "        try:\n",
    "            test(detector)\n",
    "        except Exception as e:\n",
    "            print(\"Failed test\")\n",
    "            print(e)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(net.state_dict(),\n",
    "                   os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                '_baseline_epoch_' + str(epoch) + '.pt'))\n",
    "        # Let us not waste space and delete the previous model\n",
    "        prev_path = os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                 '_baseline_epoch_' + str(epoch - 1) + '.pt')\n",
    "        if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "        # Show results\n",
    "\n",
    "        with open(os.path.join(args.save, args.dataset + '_' + args.model +\n",
    "                                          '_baseline_training_results.csv'), 'a') as f:\n",
    "            f.write('%03d,%05d,%0.6f,%0.5f,%0.2f,%0.2f\\n' % (\n",
    "                (epoch + 1),\n",
    "                time.time() - begin_epoch,\n",
    "                state['train_loss'],\n",
    "                state['test_loss'],\n",
    "                100 - 100. * state['test_accuracy'],\n",
    "                state[\"gram_auroc\"],\n",
    "            ))\n",
    "\n",
    "        # # print state with rounded decimals\n",
    "#         print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n",
    "\n",
    "        print('Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f} | Gram Auroc {5:.2f}'.format(\n",
    "            (epoch + 1),\n",
    "            int(time.time() - begin_epoch),\n",
    "            state['train_loss'],\n",
    "            state['test_loss'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "            state[\"gram_auroc\"])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
